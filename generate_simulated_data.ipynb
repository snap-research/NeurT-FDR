{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "Copyright Snap Inc. 2021. This sample code is made available by Snap Inc. for informational purposes only. \n",
    "No license, whether implied or otherwise, is granted in or to such code (including any rights to copy, modify, \n",
    "publish, distribute and/or commercialize such code), unless you have entered into a separate agreement for such rights. \n",
    "Such code is provided as-is, without warranty of any kind, express or implied, including any warranties of merchantability, \n",
    "title, fitness for a particular purpose, non-infringement, or that such code is free of defects, errors or viruses. \n",
    "In no event will Snap Inc. be liable for any damages or losses of any kind arising from the sample code or your use thereof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:44.213647Z",
     "start_time": "2021-01-07T18:07:38.129850Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "\n",
    "f0 = np.random.normal(0, 1, 1000)\n",
    "f1 = np.random.normal(0, 3, 1000)\n",
    "\n",
    "distributions = [\n",
    "    {\"type\": np.random.normal, \"kwargs\": {\"loc\": -2.5, \"scale\": 1}},\n",
    "    {\"type\": np.random.normal, \"kwargs\": {\"loc\": 2.5, \"scale\": 1}},\n",
    "]\n",
    "coefficients = np.array([0.5, 0.5])\n",
    "coefficients /= coefficients.sum()      # in case these did not add up to 1\n",
    "sample_size = 1000\n",
    "\n",
    "num_distr = len(distributions)\n",
    "data = np.zeros((sample_size, num_distr))\n",
    "for idx, distr in enumerate(distributions):\n",
    "    data[:, idx] = distr[\"type\"](size=(sample_size,), **distr[\"kwargs\"])\n",
    "random_idx = np.random.choice(np.arange(num_distr), size=(sample_size,), p=coefficients)\n",
    "f2 = data[np.arange(sample_size), random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:44.367230Z",
     "start_time": "2021-01-07T18:07:44.215705Z"
    }
   },
   "outputs": [],
   "source": [
    "p1 = sns.kdeplot(f0, shade=False, color=\"gray\")\n",
    "p1 = sns.kdeplot(f1, shade=False, color=\"blue\")\n",
    "p2 = sns.kdeplot(f2, shade=False, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:44.835608Z",
     "start_time": "2021-01-07T18:07:44.369638Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {\"Null\":f0,\n",
    "                  \"Poorly-Separated\":f1,\n",
    "                  \"Well-Separated\":f2}\n",
    "df = pd.DataFrame(data, columns = ['Null', 'Poorly-Separated','Well-Separated'])\n",
    "\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "use = [\"Null\",\"Poorly-Separated\",\"Well-Separated\"]\n",
    "for u in use:\n",
    "    sns.kdeplot(df[u], ax=ax, label=u)\n",
    "\n",
    "plt.ylabel('$f_1(z)$', fontsize=16)\n",
    "plt.xlabel('z', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate covariate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:44.846796Z",
     "start_time": "2021-01-07T18:07:44.837414Z"
    }
   },
   "outputs": [],
   "source": [
    "### continous covariates\n",
    "def generate_core_normal(n, m1, m2 ):\n",
    "    mu1 = np.random.random(m1)\n",
    "\n",
    "    cov1 = np.identity(m1) # independency\n",
    "\n",
    "    mu2 = np.random.random(m2)\n",
    "    cov2 = np.identity(m2)\n",
    "\n",
    "    x1 = np.random.multivariate_normal(mu1, cov1, 1000)\n",
    "    x2 = np.random.multivariate_normal(mu2, cov2, 1000)\n",
    "    weight1 = np.random.normal(0, 4, m1)\n",
    "    weight2 = np.random.lognormal(2,1, m2)\n",
    "\n",
    "    h1_linear = x1 * weight1 \n",
    "    h1_linear = h1_linear.sum(axis=1)\n",
    "    h1 = (1/(1+np.exp(h1_linear))) \n",
    "\n",
    "    h2_linear = x2 * weight2\n",
    "    h2_linear = h1_linear - h2_linear.sum(axis=1) \n",
    "\n",
    "    h2 = 1/(1+np.exp(h2_linear)) \n",
    "    \n",
    "    return x1, x2, h1, h2\n",
    "\n",
    "### unif covariates\n",
    "def generate_core_unif(n, m1, m2):\n",
    "    x1 = np.random.uniform(0, 1, size=(n, m1))\n",
    "    x2 = np.random.uniform(0, 1, size=(n, m2))\n",
    "\n",
    "    weight_a1 = np.random.lognormal(0, 1, m1)\n",
    "    weight_b1 = np.random.lognormal(1, 1, m1)\n",
    "\n",
    "    alpha1 =  (x1 * weight_a1).sum(axis=1)\n",
    "    beta1 = (x1 * weight_b1).sum(axis=1)\n",
    "\n",
    "    lr1 = LinearRegression()\n",
    "    lr1.fit(x2, alpha1)\n",
    "    alpha2 = lr1.predict(x2)\n",
    "\n",
    "    lr2 = LinearRegression()\n",
    "    lr2.fit(x2, beta1)\n",
    "    beta2 = lr1.predict(x2)\n",
    "\n",
    "    c1 = np.random.beta(alpha1, beta1)\n",
    "    c2 =  np.random.beta(alpha2, beta2)\n",
    "    \n",
    "    return x1, x2, c1, c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug and check the generated covariates(linear)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:44.862647Z",
     "start_time": "2021-01-07T18:07:44.848656Z"
    }
   },
   "outputs": [],
   "source": [
    "### debug covariates are generated from multivariate normal with linear realtionship\n",
    "\n",
    "m1 = 100\n",
    "m2 = 5\n",
    "\n",
    "mu1 = np.random.random(m1)\n",
    "\n",
    "cov1 = np.identity(m1) # independency\n",
    "\n",
    "mu2 = np.random.random(m2)\n",
    "cov2 = np.identity(m2)\n",
    "\n",
    "x1 = np.random.multivariate_normal(mu1, cov1, 1000)\n",
    "x2 = np.random.multivariate_normal(mu2, cov2, 1000)\n",
    "weight1 = np.random.normal(0, 1, m1)\n",
    "weight2 = np.random.normal(2,1, m2)\n",
    "\n",
    "h1_linear = x1 * weight1 \n",
    "h1_linear = h1_linear.sum(axis=1)\n",
    "h1 = (1/(1+np.exp(h1_linear))) \n",
    "\n",
    "h2_linear = x2 * weight2\n",
    "h2_linear = h1_linear - h2_linear.sum(axis=1) \n",
    "\n",
    "h2 = 1/(1+np.exp(h2_linear)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug and check the generated covariates(nonlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:44.881717Z",
     "start_time": "2021-01-07T18:07:44.865028Z"
    }
   },
   "outputs": [],
   "source": [
    "### debug covariates are generated from multivariate normal with non-linear realtionship\n",
    "\n",
    "m1 = 100\n",
    "m2 = 5\n",
    "\n",
    "mu1 = np.random.random(m1)\n",
    "\n",
    "cov1 = np.identity(m1) # independency\n",
    "\n",
    "mu2 = np.random.random(m2)\n",
    "cov2 = np.identity(m2)\n",
    "\n",
    "x1 = np.random.multivariate_normal(mu1, cov1, 1000)\n",
    "x2 = np.random.multivariate_normal(mu2, cov2, 1000)\n",
    "weight1 = np.random.normal(0, 1, m1)\n",
    "weight2 = np.random.normal(2,1, m2)\n",
    "\n",
    "h1_linear = np.square(x1) * weight1\n",
    "h1_linear = h1_linear.sum(axis=1)\n",
    "h1 = (1/(1+np.exp(h1_linear))) \n",
    "\n",
    "h2_linear = np.square(x2) * weight2\n",
    "h2_linear = h1_linear - h2_linear.sum(axis=1) \n",
    "\n",
    "h2 = 1/(1+np.exp(h2_linear)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug and check the generated covariates(constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:44.900099Z",
     "start_time": "2021-01-07T18:07:44.886448Z"
    }
   },
   "outputs": [],
   "source": [
    "m1 = 100\n",
    "m2 = 5\n",
    "\n",
    "mu1 = np.random.random(m1)\n",
    "\n",
    "cov1 = np.identity(m1) # independency\n",
    "\n",
    "mu2 = np.random.random(m2)\n",
    "cov2 = np.identity(m2)\n",
    "\n",
    "x1 = np.random.multivariate_normal(mu1, cov1, 1000)\n",
    "x2 = np.random.multivariate_normal(mu2, cov2, 1000)\n",
    "\n",
    "h2 = bernoulli.rvs(0.5, size=1000)\n",
    "\n",
    "z_ws_continous = h2 * f2 + (1 - h2) * f0\n",
    "z_ps_continous = h2 * f1 + (1 - h2) * f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:44.907422Z",
     "start_time": "2021-01-07T18:07:44.902739Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(h1>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:44.913385Z",
     "start_time": "2021-01-07T18:07:44.909757Z"
    }
   },
   "outputs": [],
   "source": [
    "h2[h2 > 0.5] = 1\n",
    "h2[h2 <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:44.920103Z",
     "start_time": "2021-01-07T18:07:44.915693Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate z\n",
    "z_ws_continous = h2 * f2 + (1 - h2) * f0\n",
    "z_ps_continous = h2 * f1 + (1 - h2) * f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:48.695315Z",
     "start_time": "2021-01-07T18:07:44.922371Z"
    }
   },
   "outputs": [],
   "source": [
    "# quick check h1 and h2\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    x1, x2, h1, h2 = generate_core_unif(1000, 100, 10)\n",
    "    \n",
    "    if np.sum(h2>0.5) > np.sum(h1>0.5):\n",
    "        count += 1\n",
    "        \n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T18:07:48.714855Z",
     "start_time": "2021-01-07T18:07:48.698051Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"data/simulation\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# save data\n",
    "dat = pd.concat([pd.DataFrame(x1),pd.DataFrame(x2), pd.DataFrame(h2), pd.DataFrame(z_ws_continous), pd.DataFrame(z_ps_continous)], axis=1).to_numpy()\n",
    "np.save(f\"{path}/nonlinear_dat(n=1000,m=100).npy\", dat)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m55"
  },
  "kernelspec": {
   "display_name": "icml_2021",
   "language": "python",
   "name": "icml_2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
